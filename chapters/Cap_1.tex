\chapter{Introducción}

En los últimos años, los Modelos de Lenguaje Grandes (LLMs, por sus siglas en inglés) han revolucionado el campo del procesamiento del lenguaje natural, demostrando capacidades sin precedentes para comprender y generar texto de manera coherente y contextualizada. Sin embargo, su eficacia está inherentemente limitada por el conocimiento estático adquirido durante su entrenamiento, lo que conduce a problemas bien documentados como las ``alucinaciones'' —respuestas plausibles pero incorrectas— y la incapacidad para acceder a información actualizada o a dominios de conocimiento privados.

Para mitigar estas limitaciones, ha surgido con fuerza el paradigma de Generación Aumentada por Recuperación (RAG, por sus siglas en inglés). RAG mejora las capacidades de los LLMs al permitirles acceder a fuentes de conocimiento externas en tiempo real, recuperando documentos o fragmentos de datos relevantes para una consulta específica y utilizándolos como contexto adicional para fundamentar sus respuestas \cite{Gan2025RAGEvaluationSurvey}. Este enfoque no solo aumenta la fiabilidad y precisión de los modelos, sino que también abre la puerta a su aplicación en entornos empresariales que manejan datos privados y dinámicos.

La creciente adopción de esta tecnología ha dado lugar al modelo de RAG-como-Servicio (RAGaaS), donde las soluciones RAG se alojan en la nube, facilitando su acceso y escalabilidad. No obstante, este modelo de despliegue introduce importantes desafíos en materia de privacidad y seguridad, ya que las consultas de los usuarios, que pueden contener información sensible, deben ser transmitidas a servidores de terceros para su procesamiento \cite{Cheng2025RemoteRAG}. Esta exposición de datos resulta inaceptable en numerosos sectores, como el gubernamental, el financiero o el sanitario, donde la confidencialidad y la soberanía de los datos son requisitos indispensables.

Es en este contexto donde surge la necesidad de soluciones RAG que puedan operar de manera segura en entornos completamente desconectados (offline), garantizando que los datos nunca abandonen la infraestructura local de una organización. Este trabajo de tesis propone y desarrolla Atenex, un framework de RAG diseñado específicamente para abordar esta brecha, ofreciendo una solución robusta, privada y eficiente para despliegues offline.

\section{Motivación y Contexto}

La evolución de los LLMs ha sido exponencial, pero sus limitaciones intrínsecas han impulsado a la comunidad científica a explorar arquitecturas que extiendan sus capacidades. La Generación Aumentada por Recuperación (RAG) se ha consolidado como la solución predominante, al integrar un recuperador de información que provee evidencia externa para que el LLM genere respuestas más fiables y actualizadas \cite{Yu2024RAGEvalSurvey}. La investigación en RAG ha avanzado rápidamente, explorando desde la optimización de los recuperadores y la fusión de resultados hasta la aplicación en dominios multimodales que incluyen texto, imágenes y video \cite{Drushchak2025mRAG, Zheng2025RAGinVision}.

Sin embargo, el paradigma RAGaaS, si bien democratiza el acceso a esta tecnología, plantea serias preocupaciones de privacidad. El trabajo de \cite{Cheng2025RemoteRAG} demuestra formalmente el riesgo de fuga de información semántica a través de las consultas y los embeddings enviados a la nube, proponiendo como solución mecanismos de privacidad diferencial que, aunque efectivos, aún se encuentran en fase de investigación.

Paralelamente, existe un creciente debate sobre la necesidad de RAG en la era de los LLMs con ventanas de contexto cada vez más largas (LC-LLMs). Estudios comparativos como el de \cite{Li2024LCvsRAG} concluyen que, si bien los LC-LLMs pueden procesar grandes volúmenes de texto, RAG sigue siendo superior en términos de eficiencia, costo computacional y precisión, ya que pre-filtra y enfoca al modelo únicamente en la información más relevante. Esta conclusión refuerza la vigencia de RAG, especialmente en entornos con recursos computacionales limitados.

Finalmente, la viabilidad de desplegar sistemas de IA complejos en hardware local y sin aceleración por GPU es un área de investigación crítica pero poco explorada. El trabajo de \cite{Tyndall2025OfflineRAG} evalúa el rendimiento de LLMs con RAG en sistemas CPU-only, concluyendo que tales despliegues son factibles para tareas estructuradas como la respuesta a preguntas factuales, aunque con limitaciones en tareas generativas más complejas como la sumarización. Este hallazgo proporciona una base empírica fundamental que motiva el desarrollo de frameworks optimizados para entornos offline como Atenex.

\section{Planteamiento del Problema}

A pesar de los beneficios demostrados de RAG, su adopción en entornos corporativos y gubernamentales con altos requisitos de seguridad se ve obstaculizada por una brecha fundamental: la dependencia de servicios en la nube y la consiguiente exposición de datos sensibles. Las soluciones RAGaaS exponen las consultas de los usuarios a proveedores externos, creando un riesgo de privacidad inaceptable para muchas organizaciones \cite{Cheng2025RemoteRAG}. Por otro lado, la implementación de un sistema RAG local, seguro, eficiente y robusto desde cero representa un desafío técnico considerable, ya que requiere la integración de múltiples componentes complejos, desde bases de datos vectoriales y recuperadores híbridos hasta la gestión segura de múltiples usuarios (multi-tenant) con aislamiento de datos.

Actualmente, no existe en el ecosistema de código abierto un framework integral que ofrezca una solución RAG offline, orientada a la privacidad y lista para su despliegue en entornos multi-tenant, que sea a la vez robusta frente a información ruidosa y computacionalmente eficiente.

La pregunta central de investigación de esta tesis es: \textbf{¿cómo se puede diseñar e implementar un framework de Generación Aumentada por Recuperación (Atenex) que opere de manera segura y eficiente en un entorno offline y multi-tenant, garantizando la privacidad de las consultas y la robustez de las respuestas, para ofrecer una alternativa viable a las soluciones RAG dependientes de la nube?}

\section{Objetivos}

El objetivo general de esta tesis es diseñar, implementar y evaluar Atenex, un framework de Generación Aumentada por Recuperación (RAG) de código abierto, enfocado en la privacidad y la seguridad para su despliegue en entornos offline y multi-tenant.

\subsection{Objetivos Específicos}

Para alcanzar el objetivo general, se plantean los siguientes objetivos específicos:

\begin{enumerate}
    \item Analizar el estado del arte de las arquitecturas RAG y RAGaaS, identificando las brechas existentes en materia de privacidad, seguridad y robustez en despliegues offline.
    \item Diseñar e implementar la arquitectura modular de Atenex, incorporando un pipeline de recuperación híbrida, mecanismos de re-ranking (reranking) y una gestión multi-tenant segura para el aislamiento de datos entre usuarios.
    \item Desarrollar y evaluar un mecanismo de ofuscación de consultas basado en la perturbación de embeddings para proteger la privacidad del usuario en el framework Atenex, midiendo el balance entre el nivel de privacidad y la precisión de la recuperación de información.
    \item Validar la robustez del sistema Atenex frente a evidencia conflictiva o ruidosa mediante la creación de un benchmark de evaluación, y proponer un mecanismo de filtrado para mitigar el impacto de la desinformación en las respuestas generadas.
    \item Realizar un análisis comparativo de rendimiento (latencia, precisión y costo computacional) entre Atenex y el enfoque de usar Modelos de Lenguaje de Gran Contexto (LC-LLMs) en tareas de pregunta-respuesta sobre un corpus documental especializado.
\end{enumerate}

\section{Organización de la tesis}

La presente tesis se estructura de la siguiente manera para abordar los objetivos planteados:

\begin{itemize}
    \item \textbf{Capítulo 1: Introducción.} Se presenta la motivación, el contexto del problema, los objetivos de la investigación y la organización general del documento.
    \item \textbf{Capítulo 2: Marco Teórico.} Se revisan los conceptos fundamentales de los Modelos de Lenguaje Grandes (LLMs), las arquitecturas de Generación Aumentada por Recuperación (RAG), las técnicas de recuperación de información, y se profundiza en los desafíos de privacidad, seguridad y robustez en dichos sistemas.
    \item \textbf{Capítulo 3: Propuesta: Atenex.} Se describe en detalle la arquitectura del framework Atenex, detallando cada uno de sus componentes, el pipeline de procesamiento de datos, el flujo de recuperación de información y el diseño del módulo de privacidad.
    \item \textbf{Capítulo 4: Pruebas y Resultados.} Se presentan la metodología experimental, los conjuntos de datos utilizados y los resultados obtenidos al evaluar la privacidad, robustez y eficiencia de Atenex. Se incluye el análisis comparativo con otros enfoques de vanguardia.
    \item \textbf{Capítulo 5: Conclusiones y Trabajos Futuros.} Se resumen los hallazgos principales de la investigación, se discuten las limitaciones del trabajo realizado y se proponen líneas de investigación futuras para extender y mejorar la propuesta.
\end{itemize}