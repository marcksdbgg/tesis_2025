# tesis_2025

Generated by generate_repo_markdown.py.

## Estructura

```
.
├── .github
│   └── copilot-instructions.md
├── .vscode
│   └── settings.json
├── bibliography
│   └── Bibliog.bib
├── chapters
│   ├── Cap_1.tex
│   ├── Cap_2.tex
│   ├── Cap_3.tex
│   ├── Cap_4.tex
│   └── conclusiones.tex
├── figs
│   ├── Chap2
│   │   └── SLAM-TopologicalRepresentation.jpg
│   └── UCSP_black.pdf
├── frontmatter
│   ├── Abstract.tex
│   ├── Agradecimientos.tex
│   ├── Resumen.tex
│   ├── _README.txt
│   ├── abreviaturas.aux
│   └── abreviaturas.tex
├── scripts
│   └── fix_quotes.py
├── .gitignore
├── Tesis.tex
└── export_codebase.py
```

## Contenido de archivos

### `.gitignore`

```gitignore
[binary file omitted – 1072 bytes]

```

### `.vscode/settings.json`

```json
{
    "workbench.colorCustomizations": {
        "activityBar.background": "#182F46",
        "titleBar.activeBackground": "#224262",
        "titleBar.activeForeground": "#F8FAFC"
    }
}

```

### `bibliography/Bibliog.bib`

```bib
% Ejemplo de Bibliografía
% Recomendado usar Encoding: UTF-8
% Se sugiere usar Jabref para gestionor sus referencias (obtener los datos correctos a partir de ISBN/ISSN o DOI)
% Se recomienda que el identificador sea Authoy_Year o Authoryear para facilidad de ver cita en el texto TeX

@article{Li2024LCvsRAG,
  title={Retrieval Augmented Generation or Long-Context LLMs? A Comprehensive Study and Hybrid Approach},
  author={Li, Zhuowan and Li, Cheng and Zhang, Mingyang and Mei, Qiaozhu and Bendersky, Michael},
  journal={Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing: Industry Track},
  pages={881--893},
  year={2024},
  organization={Association for Computational Linguistics}
}

@article{Drushchak2025mRAG,
  title={Multimodal Retrieval-Augmented Generation: Unified Information Processing Across Text, Image, Table, and Video Modalities},
  author={Drushchak, Nazarii and Polyakovska, Nataliya and Bautina, Maryna and Semenchenko, Taras and Koscielecki, Jakub and Sykala, Wojciech and Wegrzynowski, Michal},
  journal={Proceedings of the 1st Workshop on Multimodal Augmented Generation via Multimodal Retrieval (MAGMaR 2025)},
  pages={59--64},
  year={2025},
  organization={Association for Computational Linguistics}
}

@article{Yu2024RAGEvalSurvey,
  title={Evaluation of Retrieval-Augmented Generation: A Survey},
  author={Yu, Hao and Gan, Aoran and Zhang, Kai and Tong, Shiwei and Liu, Qi and Liu, Zhaofeng},
  journal={arXiv preprint arXiv:2405.07437},
  year={2024}
}

@article{Cheng2025RemoteRAG,
  title={RemoteRAG: A Privacy-Preserving LLM Cloud RAG Service},
  author={Cheng, Yihang and Zhang, Lan and Wang, Junyang and Yuan, Mu and Yao, Yunhao},
  journal={Findings of the Association for Computational Linguistics: ACL 2025},
  pages={3820--3837},
  year={2025}
}

@article{Tyndall2025OfflineRAG,
  title={Feasibility Evaluation of Secure Offline Large Language Models with Retrieval-Augmented Generation for CPU-Only Inference},
  author={Tyndall, Erick and Wagner, Torrey and Gayheart, Colleen and Some, Alexandre and Langhals, Brent},
  journal={Information},
  volume={16},
  number={9},
  pages={744},
  year={2025},
  publisher={MDPI}
}

@article{Wu2024MedicalGraphRAG,
  title={Medical Graph RAG: Towards Safe Medical Large Language Model via Graph Retrieval-Augmented Generation},
  author={Wu, Junde and Zhu, Jiayuan and Qi, Yunli and Chen, Jingkun and Xu, Min and Menolascina, Filippo and Grau, Vicente},
  journal={arXiv preprint arXiv:2408.04187},
  year={2024}
}

@article{Gan2025RAGEvaluationSurvey,
  title={Retrieval Augmented Generation Evaluation in the Era of Large Language Models: A Comprehensive Survey},
  author={Gan, Aoran and Yu, Hao and Zhang, Kai and Liu, Qi and Yan, Wenyu and Huang, Zhenya and Tong, Shiwei and Chen, Enhong and Hu, Guoping},
  journal={Frontiers of Computer Science},
  year={2025},
  note={in press}
}

@article{Zheng2025RAGinVision,
  title={Retrieval Augmented Generation and Understanding in Vision: A Survey and New Outlook},
  author={Zheng, Xu and Weng, Ziqiao and Lyu, Yuanhuiyi and Jiang, Lutao and Xue, Haiwei and Ren, Bin and Paudel, Danda and Sebe, Nicu and Van Gool, Luc and Hu, Xuming},
  journal={arXiv preprint arXiv:2503.18016},
  year={2025}
}

@article{Angeli2008,
  author  = {Angeli, A. and Filliat, D. and Doncieux, S. and Meyer, J.},
  title   = {Fast and Incremental Method for Loop-Closure Detection Using Bags of Visual Words},
  journal = {IEEE Transactions on Robotics},
  year    = {2008},
  volume  = {24},
  number  = {5},
  pages   = {1027--1037},
  doi     = {10.1109/TRO.2008.2004514},
}

@article{Choset_2001,
  author  = {Choset, H. and Nagatani, K.},
  title   = {Topological simultaneous localization and mapping (SLAM): toward exact localization without explicit localization},
  journal = {IEEE Transactions on Robotics and Automation},
  year    = {2001},
  volume  = {17},
  number  = {2},
  pages   = {125--137},
}

@PHDTHESIS{Galante01,
  author = {Galante, R.},
  title  = {Evolución de Esquemas en Bancos de Datos Orientados a Objetos con el empleo de versiones},
  school = {Instituto de Informática-UFRGS},
  year   = {2001},
}

@ARTICLE{Mateos00,
  author  = {Mateos, G. and García, M. and Ortín, M.},
  title   = {Inclusión de vistas en ODMG},
  journal = {Jornadas de Ingeniería del Software y Bases de Datos (JISBD 2000)},
  pages   = {383--395},
  year    = {2000},
}

```

### `chapters/Cap_1.tex`

```tex
\chapter{Introducción}

En los últimos años, los Modelos de Lenguaje Grandes (LLMs, por sus siglas en inglés) han revolucionado el campo del procesamiento del lenguaje natural, demostrando capacidades sin precedentes para comprender y generar texto de manera coherente y contextualizada. Sin embargo, su eficacia está inherentemente limitada por el conocimiento estático adquirido durante su entrenamiento, lo que conduce a problemas bien documentados como las ``alucinaciones'' —respuestas plausibles pero incorrectas— y la incapacidad para acceder a información actualizada o a dominios de conocimiento privados.

Para mitigar estas limitaciones, ha surgido con fuerza el paradigma de Generación Aumentada por Recuperación (RAG, por sus siglas en inglés). RAG mejora las capacidades de los LLMs al permitirles acceder a fuentes de conocimiento externas en tiempo real, recuperando documentos o fragmentos de datos relevantes para una consulta específica y utilizándolos como contexto adicional para fundamentar sus respuestas \cite{Gan2025RAGEvaluationSurvey}. Este enfoque no solo aumenta la fiabilidad y precisión de los modelos, sino que también abre la puerta a su aplicación en entornos empresariales que manejan datos privados y dinámicos.

La creciente adopción de esta tecnología ha dado lugar al modelo de RAG-como-Servicio (RAGaaS), donde las soluciones RAG se alojan en la nube, facilitando su acceso y escalabilidad. No obstante, este modelo de despliegue introduce importantes desafíos en materia de privacidad y seguridad, ya que las consultas de los usuarios, que pueden contener información sensible, deben ser transmitidas a servidores de terceros para su procesamiento \cite{Cheng2025RemoteRAG}. Esta exposición de datos resulta inaceptable en numerosos sectores, como el gubernamental, el financiero o el sanitario, donde la confidencialidad y la soberanía de los datos son requisitos indispensables.

Es en este contexto donde surge la necesidad de soluciones RAG que puedan operar de manera segura en entornos completamente desconectados (offline), garantizando que los datos nunca abandonen la infraestructura local de una organización. Este trabajo de tesis propone y desarrolla Atenex, un framework de RAG diseñado específicamente para abordar esta brecha, ofreciendo una solución robusta, privada y eficiente para despliegues offline.

\section{Motivación y Contexto}

La evolución de los LLMs ha sido exponencial, pero sus limitaciones intrínsecas han impulsado a la comunidad científica a explorar arquitecturas que extiendan sus capacidades. La Generación Aumentada por Recuperación (RAG) se ha consolidado como la solución predominante, al integrar un recuperador de información que provee evidencia externa para que el LLM genere respuestas más fiables y actualizadas \cite{Yu2024RAGEvalSurvey}. La investigación en RAG ha avanzado rápidamente, explorando desde la optimización de los recuperadores y la fusión de resultados hasta la aplicación en dominios multimodales que incluyen texto, imágenes y video \cite{Drushchak2025mRAG, Zheng2025RAGinVision}.

Sin embargo, el paradigma RAGaaS, si bien democratiza el acceso a esta tecnología, plantea serias preocupaciones de privacidad. El trabajo de \cite{Cheng2025RemoteRAG} demuestra formalmente el riesgo de fuga de información semántica a través de las consultas y los embeddings enviados a la nube, proponiendo como solución mecanismos de privacidad diferencial que, aunque efectivos, aún se encuentran en fase de investigación.

Paralelamente, existe un creciente debate sobre la necesidad de RAG en la era de los LLMs con ventanas de contexto cada vez más largas (LC-LLMs). Estudios comparativos como el de \cite{Li2024LCvsRAG} concluyen que, si bien los LC-LLMs pueden procesar grandes volúmenes de texto, RAG sigue siendo superior en términos de eficiencia, costo computacional y precisión, ya que pre-filtra y enfoca al modelo únicamente en la información más relevante. Esta conclusión refuerza la vigencia de RAG, especialmente en entornos con recursos computacionales limitados.

Finalmente, la viabilidad de desplegar sistemas de IA complejos en hardware local y sin aceleración por GPU es un área de investigación crítica pero poco explorada. El trabajo de \cite{Tyndall2025OfflineRAG} evalúa el rendimiento de LLMs con RAG en sistemas CPU-only, concluyendo que tales despliegues son factibles para tareas estructuradas como la respuesta a preguntas factuales, aunque con limitaciones en tareas generativas más complejas como la sumarización. Este hallazgo proporciona una base empírica fundamental que motiva el desarrollo de frameworks optimizados para entornos offline como Atenex.

\section{Planteamiento del Problema}

A pesar de los beneficios demostrados de RAG, su adopción en entornos corporativos y gubernamentales con altos requisitos de seguridad se ve obstaculizada por una brecha fundamental: la dependencia de servicios en la nube y la consiguiente exposición de datos sensibles. Las soluciones RAGaaS exponen las consultas de los usuarios a proveedores externos, creando un riesgo de privacidad inaceptable para muchas organizaciones \cite{Cheng2025RemoteRAG}. Por otro lado, la implementación de un sistema RAG local, seguro, eficiente y robusto desde cero representa un desafío técnico considerable, ya que requiere la integración de múltiples componentes complejos, desde bases de datos vectoriales y recuperadores híbridos hasta la gestión segura de múltiples usuarios (multi-tenant) con aislamiento de datos.

Actualmente, no existe en el ecosistema de código abierto un framework integral que ofrezca una solución RAG offline, orientada a la privacidad y lista para su despliegue en entornos multi-tenant, que sea a la vez robusta frente a información ruidosa y computacionalmente eficiente.

La pregunta central de investigación de esta tesis es: \textbf{¿cómo se puede diseñar e implementar un framework de Generación Aumentada por Recuperación (Atenex) que opere de manera segura y eficiente en un entorno offline y multi-tenant, garantizando la privacidad de las consultas y la robustez de las respuestas, para ofrecer una alternativa viable a las soluciones RAG dependientes de la nube?}

\section{Objetivos}

El objetivo general de esta tesis es diseñar, implementar y evaluar Atenex, un framework de Generación Aumentada por Recuperación (RAG) de código abierto, enfocado en la privacidad y la seguridad para su despliegue en entornos offline y multi-tenant.

\subsection{Objetivos Específicos}

Para alcanzar el objetivo general, se plantean los siguientes objetivos específicos:

\begin{enumerate}
    \item Analizar el estado del arte de las arquitecturas RAG y RAGaaS, identificando las brechas existentes en materia de privacidad, seguridad y robustez en despliegues offline.
    \item Diseñar e implementar la arquitectura modular de Atenex, incorporando un pipeline de recuperación híbrida, mecanismos de re-ranking (reranking) y una gestión multi-tenant segura para el aislamiento de datos entre usuarios.
    \item Desarrollar y evaluar un mecanismo de ofuscación de consultas basado en la perturbación de embeddings para proteger la privacidad del usuario en el framework Atenex, midiendo el balance entre el nivel de privacidad y la precisión de la recuperación de información.
    \item Validar la robustez del sistema Atenex frente a evidencia conflictiva o ruidosa mediante la creación de un benchmark de evaluación, y proponer un mecanismo de filtrado para mitigar el impacto de la desinformación en las respuestas generadas.
    \item Realizar un análisis comparativo de rendimiento (latencia, precisión y costo computacional) entre Atenex y el enfoque de usar Modelos de Lenguaje de Gran Contexto (LC-LLMs) en tareas de pregunta-respuesta sobre un corpus documental especializado.
\end{enumerate}

\section{Organización de la tesis}

La presente tesis se estructura de la siguiente manera para abordar los objetivos planteados:

\begin{itemize}
    \item \textbf{Capítulo 1: Introducción.} Se presenta la motivación, el contexto del problema, los objetivos de la investigación y la organización general del documento.
    \item \textbf{Capítulo 2: Marco Teórico.} Se revisan los conceptos fundamentales de los Modelos de Lenguaje Grandes (LLMs), las arquitecturas de Generación Aumentada por Recuperación (RAG), las técnicas de recuperación de información, y se profundiza en los desafíos de privacidad, seguridad y robustez en dichos sistemas.
    \item \textbf{Capítulo 3: Propuesta: Atenex.} Se describe en detalle la arquitectura del framework Atenex, detallando cada uno de sus componentes, el pipeline de procesamiento de datos, el flujo de recuperación de información y el diseño del módulo de privacidad.
    \item \textbf{Capítulo 4: Pruebas y Resultados.} Se presentan la metodología experimental, los conjuntos de datos utilizados y los resultados obtenidos al evaluar la privacidad, robustez y eficiencia de Atenex. Se incluye el análisis comparativo con otros enfoques de vanguardia.
    \item \textbf{Capítulo 5: Conclusiones y Trabajos Futuros.} Se resumen los hallazgos principales de la investigación, se discuten las limitaciones del trabajo realizado y se proponen líneas de investigación futuras para extender y mejorar la propuesta.
\end{itemize}

```

### `chapters/Cap_2.tex`

```tex
\chapter{Nombre del Capítulo II (Usualmente Marco Teórico)}\label{chap:background}

Cada capítulo deberá contener una breve introducción que describe en forma rápida el contenido del
mismo. En este capítulo va el marco teórico. (pueden ser dos capítulos de marco teórico)

\section{Sección 1 del Capítulo II}

Un capítulo puede contener $n$ secciones. 

Con respecto a las referencias bibliográficas, se hace de la siguiente manera \cite{Mateos00}, se debe de redactar el texto de forma tal que si se retiraran las referencias, la redacción no debe perjudicarse. Por ejemplo 

\subsubsection{Localización y Mapeo Simultaneos SLAM puramente topológico}
Se puede hacer un sistema SLAM con sólo reconocimiento de lugares, generando un SLAM topológico con representación basada en grafos \cite{Choset_2001}. Se almacena un registro de lugares visitados y cómo se llega de uno a otro (conexiones) sin tener información geométrica explícita.

Este tipo de SLAM es más adecuado para planificación y navegación. La figura \ref{Fig:SLAMTopological} ilustra este tipo de SLAM 
    
\begin{figure}[ht]
\centering
% placeholder figure (original content omitted in template)
\fbox{\parbox[c][4cm][c]{8cm}{\centering SLAM puramente topológico (figura de ejemplo)}}
\caption{SLAM puramente topológico basado en grafos}
\label{Fig:SLAMTopological}
\end{figure}


\subsection{Sub Sección}

Una sección puede contener $n$ sub secciones.\cite{Galante01}

\subsubsection{Sub sub sección}

Una sub sección puede contener n sub sub secciones.

\section{Recomendaciones generales de escritura}
Un trabajo de esta naturaleza debe tener en consideración varios aspectos generales:

\begin{itemize}
\item Ir de lo genérico a lo específico. Siempre hay que considerar que el lector podría ser alguien no muy familiar con el tema
	y la lectura debe serle atractiva.
\item No redactar frases muy largas. Si las frases tienen más de 2 líneas continuas es probable que la lectura sea dificultosa.
\end{itemize}

Cada capítulo excepto el primero debe contener, al finalizar, una sección de consideraciones que enlacen el presente capítulo con el siguiente.

```

### `chapters/Cap_3.tex`

```tex
\chapter{Propuesta}\label{chap:proposal}

En este capítulo se desarrolla toda la propuesta realizada a través de la investigación. Sigue la misma estructura del capítulo anterior.

El título del capítulo es flexible de acuerdo a cada tesis. Algunos títulos sugeridos podrían ser:

\begin{itemize}
\item Algoritmo X: nuestra propuesta.
\item Modelo MRLO
\end{itemize}

Este título debe de ser definido junto a su asesor de tesis. Consúltelo en su sala de clase.

```

### `chapters/Cap_4.tex`

```tex
\chapter{Pruebas y Resultados}

```

### `chapters/conclusiones.tex`

```tex
\chapter{Conclusiones y Trabajos Futuros}\label{chap:conclusiones}

Las conclusiones de la tesis son una parte muy importante y tiene las siguientes partes.

Escribir las conclusiones de su trabajo por cada uno de los objetivos específicos definidos en el Capítulo 1 y luego escribir la conclusión general de tu trabajo.

\section{Problemas encontrados}
La segunda  parte de este capítulo corresponde a los problemas encontrados, esta sección es muy importante para que los siguientes estudiantes o investigadores que hagan algo en esta línea no cometan los mismos errores y tu tesis sea un buen peldaño para avanzar más rápido.

\section{Recomendaciones}
En esta sección el tesista debe reflejar que la tesis ha permitido adquirir nuevos conocimientos que podrían servir para guiar otros trabajos en el futuro.

\section{Trabajos futuros}
En base a los puntos anteriores es recomendable que tu tesis también sugiera trabajos futuros. Esta sección es esecialmente útil para otras ideas de tesis.

Todo este capítulo no debe ser más de unas 4 páginas.

```

### `export_codebase.py`

```py
from pathlib import Path

# Carpetas que queremos excluir al recorrer la codebase
EXCLUDED_DIRS = {'.git', '__pycache__', '.venv', '.idea', '.mypy_cache', '.vscode', '.github', 'node_modules'}

def build_tree(directory: Path, prefix: str = "") -> list:
    """
    Genera una representación en árbol de la estructura de directorios y archivos,
    excluyendo las carpetas especificadas en EXCLUDED_DIRS.
    """
    # Filtrar y ordenar los elementos del directorio
    entries = sorted(
        [entry for entry in directory.iterdir() if entry.name not in EXCLUDED_DIRS],
        key=lambda e: e.name
    )
    tree_lines = []
    for index, entry in enumerate(entries):
        connector = "└── " if index == len(entries) - 1 else "├── "
        tree_lines.append(prefix + connector + entry.name)
        if entry.is_dir():
            extension = "    " if index == len(entries) - 1 else "│   "
            tree_lines.extend(build_tree(entry, prefix + extension))
    return tree_lines

def generate_codebase_markdown(base_path: str = ".", output_file: str = "full_codebase.md"):
    base = Path(base_path).resolve()

    # Si existe un paquete con el nombre del proyecto, preferirlo
    candidates = [p for p in base.iterdir() if p.is_dir() and p.name not in EXCLUDED_DIRS]
    preferred = None
    for c in candidates:
        if c.name == "atenex_offline":
            preferred = c
            break

    # Fallback: usar la carpeta preferida si existe, o la raíz del repo
    if preferred:
        app_dir = preferred
    else:
        # Si no hay una carpeta específica, usar la raíz (base)
        app_dir = base

    lines = []

    # Agregar la estructura de directorios al inicio del Markdown
    lines.append("# Estructura de la Codebase")
    lines.append("")
    lines.append("```")
    lines.append(f"{app_dir.name}/")
    tree_lines = build_tree(app_dir)
    lines.extend(tree_lines)
    lines.append("```")
    lines.append("")

    # Agregar el contenido de la codebase en Markdown
    lines.append(f"# Codebase: `{app_dir.name}`")
    lines.append("")

    # Recorrer solo la carpeta app
    for path in sorted(app_dir.rglob("*")):
        # Ignorar directorios excluidos
        if any(part in EXCLUDED_DIRS for part in path.parts):
            continue

        if path.is_file():
            rel_path = path.relative_to(base)
            lines.append(f"## File: `{rel_path}`")
            try:
                content = path.read_text(encoding='utf-8')
            except UnicodeDecodeError:
                lines.append("_[Skipped: binary or non-UTF8 file]_")
                continue
            except Exception as e:
                lines.append(f"_[Error al leer el archivo: {e}]_")
                continue
            ext = path.suffix.lstrip('.')
            lang = ext if ext else ""
            lines.append(f"```{lang}")
            lines.append(content)
            lines.append("```")
            lines.append("")

    # Agregar pyproject.toml si existe en la raíz
    toml_path = base / "pyproject.toml"
    if toml_path.exists():
        lines.append("## File: `pyproject.toml`")
        try:
            content = toml_path.read_text(encoding='utf-8')
        except UnicodeDecodeError:
            lines.append("_[Skipped: binary or non-UTF8 file]_")
        except Exception as e:
            lines.append(f"_[Error al leer el archivo: {e}]_")
        else:
            lines.append("```toml")
            lines.append(content)
            lines.append("```")
            lines.append("")

    output_path = base / output_file
    try:
        output_path.write_text("\n".join(lines), encoding='utf-8')
        print(f"[OK] Código exportado a Markdown en: {output_path}")
    except Exception as e:
        print(f"[ERROR] Error al escribir el archivo de salida: {e}")

# Si se corre el script directamente
if __name__ == "__main__":
    generate_codebase_markdown()

```

### `figs/Chap2/SLAM-TopologicalRepresentation.jpg`

```jpg
[binary file omitted – 85717 bytes]

```

### `figs/UCSP_black.pdf`

```pdf
[binary file omitted – 17870 bytes]

```

### `frontmatter/_README.txt`

```txt
This folder contains front-matter TeX files: abreviaturas.tex, Abstract.tex, Agradecimientos.tex, Resumen.tex

Do not edit the originals in project root until main file updated. Update `Tesis.tex` includes to point here.

```

### `frontmatter/abreviaturas.aux`

```aux
\relax 
\newacro{SPC}[\AC@hyperlink{SPC}{SPC}]{Sociedad Peruana de Computación}
\newacro{CMM}[\AC@hyperlink{CMM}{CMM}]{\textit  {Capability Maturity Model}}
\@setckpt{frontmatter/abreviaturas}{
\setcounter{page}{4}
\setcounter{equation}{0}
\setcounter{enumi}{0}
\setcounter{enumii}{0}
\setcounter{enumiii}{0}
\setcounter{enumiv}{0}
\setcounter{footnote}{0}
\setcounter{mpfootnote}{0}
\setcounter{part}{0}
\setcounter{chapter}{0}
\setcounter{section}{0}
\setcounter{subsection}{0}
\setcounter{subsubsection}{0}
\setcounter{paragraph}{0}
\setcounter{subparagraph}{0}
\setcounter{figure}{0}
\setcounter{table}{0}
}

```

### `frontmatter/abreviaturas.tex`

```tex
%mayores detalles de como usas las abreviaturas (acronimos)
% vea: http://www.ctan.org/tex-archive/macros/latex/contrib/acronym/
% hay un manual en pdf en esa misma direccion

\chapter*{Abreviaturas}

\begin{acronym}
\acro{SPC}{Sociedad Peruana de Computación}
\acro{CMM}{\textit{Capability Maturity Model}}
\end{acronym}

```

### `frontmatter/Abstract.tex`

```tex
\begin{abstract}
Here you should enter up to 300 words maximum, describing the problem you are trying to solve, the justification, contributions or solutions you are proposing, how much you have achieved them in terms of obtained outcomes.
\end{abstract}

```

### `frontmatter/Agradecimientos.tex`

```tex
\begin{agradecimientos}
Aquí deberás colocar a quien y porque agradeces. Ejemplo:

En primer lugar deseo agradecer a Dios por haberme guiado a lo largo de estos cinco años de estudio.

Agradezco a mis padres por el apoyo brindado para forjarme como un profesional.

Agradezco a la universidad, mi \textit{alma matter}, por haberme cobijado y brindado la formación que ahora me permitirá ayudar a construir una mejor sociedad.

Agradezco de forma muy especial a mi asesor Prof. Dr./Mag. nombre 1 por haberme guiado en esta tesis. ...

Deseo agradecer de forma especial a mis profesores: nombre 1, nombre 2, nombre 3 porque fueron ejemplos que deseo seguir en mi vida profesional.

Deseo agradecer al personal administrativo de la universidad: nombre 1, nombre 2, nombre 3. Muchas gracias por la atención brindada y porque siempre estuvieron dispuestas a ayudarnos.
\end{agradecimientos}

```

### `frontmatter/Resumen.tex`

```tex
\begin{resumen}
Aquí deberás colocar hasta 300 palabras como máximo, describiendo el problema que intentas resolver, la justificación, aportes o soluciones que planteas, qué tanto los haz alcanzado en calidad de resultados obtenidos.
\end{resumen}

```

### `scripts/fix_quotes.py`

```py
"""Normalize quotation marks in .tex files to LaTeX conventions.

This script finds straight ASCII double quotes and common Unicode “smart”
quotes and replaces them with LaTeX-style opening `` and closing '' marks.
It operates on .tex files under the given root (default: repository root).
Backups are written with a .bak extension next to each modified file.

Usage: python scripts/fix_quotes.py [--root ROOT]
"""
from pathlib import Path
import re
import argparse


REPLACEMENTS = [
    # Common Unicode smart quotes to ASCII double quote
    (re.compile(r'[\u201C\u201D\u201E\u201F]'), '"'),
    # Left single smart quote and right single smart quote -> ASCII '
    (re.compile(r"[\u2018\u2019\u201A\u201B]"), "'")
]

DOUBLE_QUOTE_PATTERN = re.compile(r'"(.*?)"', flags=re.DOTALL)


def normalize_text(text: str) -> str:
    # First normalize smart quotes to ASCII equivalents
    for pat, repl in REPLACEMENTS:
        text = pat.sub(repl, text)

    # Then convert ASCII double-quoted segments to LaTeX quotes
    def repl(m: re.Match) -> str:
        inner = m.group(1)
        return '``' + inner + "''"

    return DOUBLE_QUOTE_PATTERN.sub(repl, text)


def process_file(path: Path) -> bool:
    text = path.read_text(encoding='utf-8')
    new = normalize_text(text)
    if new != text:
        bak = path.with_suffix(path.suffix + '.bak')
        bak.write_text(text, encoding='utf-8')
        path.write_text(new, encoding='utf-8')
        return True
    return False


def find_tex_files(root: Path):
    for p in root.rglob('*.tex'):
        yield p


def main():
    parser = argparse.ArgumentParser()
    parser.add_argument('--root', default='.', help='Repository root path')
    args = parser.parse_args()
    root = Path(args.root).resolve()

    modified = []
    for f in find_tex_files(root):
        try:
            if process_file(f):
                modified.append(str(f.relative_to(root)))
        except Exception as e:
            print(f"Error processing {f}: {e}")

    if modified:
        print('Modified files:')
        for m in modified:
            print(' -', m)
    else:
        print('No changes made.')


if __name__ == '__main__':
    main()

```

### `Tesis.tex`

```tex
% Template LaTeX Tesis Ciencia de la Computación UCSP
% 2022

\documentclass[a4paper,openany,12pt]{book}
\usepackage[utf8]{inputenc}
\usepackage{graphicx}
\usepackage[spanish,mexico]{babel}
% custom style files are in the project's sty/ folder
\usepackage{sty/fancyhdr}
\usepackage{ae}
\usepackage[left=2.5cm,right=2.5cm,top=3cm,bottom=2cm]{geometry}
\usepackage[printonlyused]{acronym}
\usepackage{xspace}
\usepackage{sty/hlundef}
\usepackage{sty/tesis}
\usepackage{setspace}
\usepackage{booktabs}
% removed an empty \usepackage{} which caused a LaTeX error

\title{** Título de la Tesis **}
\title{Una Estructura de Datos para la Consulta Visual Interactiva por Similitud de Grandes Volúmenes de Datos Multidimensionales Mixtos Georeferenciados}

\author{** Nombre completo del tesista **}


\advisor{** Dr./Mg. Nombre completo del Asesor **}


\date{** Arequipa, Mes Año **}

%\examinerone{Prof. Dr. Hidalgo Buena Gente}{Presidente}%
%\examinertwo{Prof. Dr. Antero A. Gal Oppe}{Secretario}%
%\examinerthree{Prof. Dr. Liu Xiao Ling}{University of ...} % of being the case

\dedicado{Aquí deberás colocar a quien va dedicada tu tesis por ejemplo: A Dios, por todo lo que me ha dado, a todos los profesores por sus enseñanzas y algunos amigos.}

\begin{document}
\pagestyle{fancy}

\maketitle %Compone la carátula y la dedicatoria
\newpage

%\approved{\tres}%  {\tres} or {\cuatro}

% Front matter is located in frontmatter/ folder
% use \input so aux files are written alongside the main document
\input{frontmatter/abreviaturas}

\input{frontmatter/Agradecimientos} %Inserta los agradecimientos
\input{frontmatter/Resumen} %Inserta el resumen
\input{frontmatter/Abstract} %Inserta el abstract

\pagenumbering{roman}
\setcounter{page}{1}
\pagestyle{plain}

\tableofcontents %Inserta el índice general
\listoftables %Inserta el índice de cuadros
\listoffigures %Inserta el índice de figuras

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%   En esta parte deberas incluir los archivos de tu tesis   %%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\pagestyle{plain}
\pagenumbering{arabic}
\setcounter{page}{1}
% chapters are in the chapters/ folder
\input{chapters/Cap_1} %Inserta el capítulo 1
\input{chapters/Cap_2} %Inserta el capítulo 2
\input{chapters/Cap_3} %Inserta el capítulo 3
\input{chapters/Cap_4} %Inserta el capítulo 4
\input{chapters/conclusiones} %Inserta el capítulo 5

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\bibliographystyle{apalike}

% bibliography file is in the bibliography/ directory
\bibliography{bibliography/Bibliog}
\addcontentsline{toc}{chapter}{Bibliografía}
\end{document}

```
