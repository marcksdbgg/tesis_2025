# tesis_2025

Generated by generate_repo_markdown.py.

## Estructura

```
.
├── .github
│   └── copilot-instructions.md
├── .vscode
│   └── settings.json
├── bibliography
│   └── Bibliog.bib
├── chapters
│   ├── Cap_1.tex
│   ├── Cap_2.tex
│   ├── Cap_3.tex
│   ├── Cap_4.tex
│   └── conclusiones.tex
├── figs
│   ├── Chap2
│   │   └── SLAM-TopologicalRepresentation.jpg
│   └── UCSP_black.pdf
├── frontmatter
│   ├── Abstract.tex
│   ├── Agradecimientos.tex
│   ├── Resumen.tex
│   ├── _README.txt
│   ├── abreviaturas.aux
│   └── abreviaturas.tex
├── scripts
│   └── fix_quotes.py
├── .gitignore
├── Tesis.tex
└── export_codebase.py
```

## Contenido de archivos

### `.gitignore`

```gitignore
[binary file omitted – 1072 bytes]

```

### `.vscode/settings.json`

```json
{
    "workbench.colorCustomizations": {
        "activityBar.background": "#182F46",
        "titleBar.activeBackground": "#224262",
        "titleBar.activeForeground": "#F8FAFC"
    }
}

```

### `bibliography/Bibliog.bib`

```bib
% Ejemplo de Bibliografía
% Recomendado usar Encoding: UTF-8
% Se sugiere usar Jabref para gestionor sus referencias (obtener los datos correctos a partir de ISBN/ISSN o DOI)
% Se recomienda que el identificador sea Authoy_Year o Authoryear para facilidad de ver cita en el texto TeX

@article{Li2024LCvsRAG,
  title={Retrieval Augmented Generation or Long-Context LLMs? A Comprehensive Study and Hybrid Approach},
  author={Li, Zhuowan and Li, Cheng and Zhang, Mingyang and Mei, Qiaozhu and Bendersky, Michael},
  journal={Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing: Industry Track},
  pages={881--893},
  year={2024},
  organization={Association for Computational Linguistics}
}

@article{Drushchak2025mRAG,
  title={Multimodal Retrieval-Augmented Generation: Unified Information Processing Across Text, Image, Table, and Video Modalities},
  author={Drushchak, Nazarii and Polyakovska, Nataliya and Bautina, Maryna and Semenchenko, Taras and Koscielecki, Jakub and Sykala, Wojciech and Wegrzynowski, Michal},
  journal={Proceedings of the 1st Workshop on Multimodal Augmented Generation via Multimodal Retrieval (MAGMaR 2025)},
  pages={59--64},
  year={2025},
  organization={Association for Computational Linguistics}
}

@article{Yu2024RAGEvalSurvey,
  title={Evaluation of Retrieval-Augmented Generation: A Survey},
  author={Yu, Hao and Gan, Aoran and Zhang, Kai and Tong, Shiwei and Liu, Qi and Liu, Zhaofeng},
  journal={arXiv preprint arXiv:2405.07437},
  year={2024}
}

@article{Cheng2025RemoteRAG,
  title={RemoteRAG: A Privacy-Preserving LLM Cloud RAG Service},
  author={Cheng, Yihang and Zhang, Lan and Wang, Junyang and Yuan, Mu and Yao, Yunhao},
  journal={Findings of the Association for Computational Linguistics: ACL 2025},
  pages={3820--3837},
  year={2025}
}

@article{Tyndall2025OfflineRAG,
  title={Feasibility Evaluation of Secure Offline Large Language Models with Retrieval-Augmented Generation for CPU-Only Inference},
  author={Tyndall, Erick and Wagner, Torrey and Gayheart, Colleen and Some, Alexandre and Langhals, Brent},
  journal={Information},
  volume={16},
  number={9},
  pages={744},
  year={2025},
  publisher={MDPI}
}

@article{Wu2024MedicalGraphRAG,
  title={Medical Graph RAG: Towards Safe Medical Large Language Model via Graph Retrieval-Augmented Generation},
  author={Wu, Junde and Zhu, Jiayuan and Qi, Yunli and Chen, Jingkun and Xu, Min and Menolascina, Filippo and Grau, Vicente},
  journal={arXiv preprint arXiv:2408.04187},
  year={2024}
}

@article{Gan2025RAGEvaluationSurvey,
  title={Retrieval Augmented Generation Evaluation in the Era of Large Language Models: A Comprehensive Survey},
  author={Gan, Aoran and Yu, Hao and Zhang, Kai and Liu, Qi and Yan, Wenyu and Huang, Zhenya and Tong, Shiwei and Chen, Enhong and Hu, Guoping},
  journal={Frontiers of Computer Science},
  year={2025},
  note={in press}
}

@article{Zheng2025RAGinVision,
  title={Retrieval Augmented Generation and Understanding in Vision: A Survey and New Outlook},
  author={Zheng, Xu and Weng, Ziqiao and Lyu, Yuanhuiyi and Jiang, Lutao and Xue, Haiwei and Ren, Bin and Paudel, Danda and Sebe, Nicu and Van Gool, Luc and Hu, Xuming},
  journal={arXiv preprint arXiv:2503.18016},
  year={2025}
}

@article{Angeli2008,
  author  = {Angeli, A. and Filliat, D. and Doncieux, S. and Meyer, J.},
  title   = {Fast and Incremental Method for Loop-Closure Detection Using Bags of Visual Words},
  journal = {IEEE Transactions on Robotics},
  year    = {2008},
  volume  = {24},
  number  = {5},
  pages   = {1027--1037},
  doi     = {10.1109/TRO.2008.2004514},
}

@article{Choset_2001,
  author  = {Choset, H. and Nagatani, K.},
  title   = {Topological simultaneous localization and mapping (SLAM): toward exact localization without explicit localization},
  journal = {IEEE Transactions on Robotics and Automation},
  year    = {2001},
  volume  = {17},
  number  = {2},
  pages   = {125--137},
}

@PHDTHESIS{Galante01,
  author = {Galante, R.},
  title  = {Evolución de Esquemas en Bancos de Datos Orientados a Objetos con el empleo de versiones},
  school = {Instituto de Informática-UFRGS},
  year   = {2001},
}

@ARTICLE{Mateos00,
  author  = {Mateos, G. and García, M. and Ortín, M.},
  title   = {Inclusión de vistas en ODMG},
  journal = {Jornadas de Ingeniería del Software y Bases de Datos (JISBD 2000)},
  pages   = {383--395},
  year    = {2000},
}

@article{Lewis2020RAG,
  title={Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks},
  author={Lewis, Patrick and Perez, Ethan and Ott, Myle and Levy, Omer and Stoyanov, Veselin and Kiela, Douwe},
  journal={Advances in Neural Information Processing Systems},
  year={2020},
  url={https://proceedings.neurips.cc/paper/2020/file/6b493230205f780e1bc26945df7481e5-Paper.pdf}
}

@article{Guu2020REALM,
  title={REALM: Retrieval-Augmented Language Model Pre-Training},
  author={Guu, Kelvin and Lee, Kenton and Tung, Zora and Pasupat, Panupong and Chang, Ming-Wei},
  journal={Proceedings of the International Conference on Machine Learning},
  year={2020},
  url={http://proceedings.mlr.press/v119/guu20a/guu20a.pdf}
}

@inproceedings{Chan2024RQ-RAG,
  title={RQ-RAG: Learning to Refine Queries for Retrieval Augmented Generation},
  author={Chan, Yilin and others},
  booktitle={ACL 2024},
  year={2024},
  url={https://arxiv.org/pdf/2401.00886.pdf}
}

@article{Zhang2025LevelRAG,
  title={LevelRAG: Enhancing Retrieval-Augmented Generation with Multi-hop Logic Planning over Rewriting Augmented Searchers},
  author={Zhang, Zhuocheng and Feng, Yang and Zhang, Min},
  journal={arXiv preprint arXiv:2502.18139},
  year={2025},
  url={https://arxiv.org/pdf/2502.18139.pdf}
}

@article{Wang2025HopRAG,
  title={HopRAG: Multi-Hop Reasoning for Logic-Aware Retrieval-Augmented Generation},
  author={Wang, Zhengren and others},
  journal={arXiv preprint arXiv:2502.12442},
  year={2025},
  url={https://arxiv.org/pdf/2502.12442.pdf}
}

@article{Wang2023FILCO,
  title={Learning to Filter Context for Retrieval-Augmented Generation},
  author={Wang, Kai and Chen, Yankai and Shen, Yanyan and others},
  journal={arXiv preprint arXiv:2305.17921},
  year={2023},
  url={https://arxiv.org/pdf/2305.17921.pdf}
}

@inproceedings{Asai2023SelfRAG,
  title={Self-RAG: Learning to Retrieve, Generate, and Critique through Self-Reflection},
  author={Asai, Akari and Yan, Junpeng and Zheng, Yihan and others},
  booktitle={EMNLP 2023},
  year={2023},
  url={https://arxiv.org/pdf/2309.03884.pdf}
}

@inproceedings{Cheng2023SelfMem,
  title={Lift Yourself Up: Retrieval-Augmented Text Generation with Self Memory},
  author={Cheng, Xin and Luo, Di and Chen, Xiuying and others},
  booktitle={NeurIPS 2023},
  year={2023},
  url={https://arxiv.org/pdf/2305.02437.pdf}
}

@inproceedings{Li2025BRIEF,
  title={BRIEF: Bridging Retrieval and Inference for Multi-hop Reasoning via Compression},
  author={Li, Yuankai and Gu, Jia-Chen and Wu, Di and Chang, Kai-Wei and Peng, Nanyun},
  booktitle={Findings of NAACL 2025},
  year={2025},
  url={https://aclanthology.org/2025.findings-naacl.301.pdf}
}

@article{Yang2024IM-RAG,
  title={IM-RAG: Multi-Round Retrieval-Augmented Generation through Learning Inner Monologues},
  author={Yang, Jiecao and others},
  journal={arXiv preprint arXiv:2401.10666},
  year={2024},
  url={https://arxiv.org/pdf/2401.10666.pdf}
}

@article{Wang2025CoRAG,
  title={Chain-of-Retrieval Augmented Generation},
  author={Wang, David and others},
  journal={arXiv preprint arXiv:2406.15142},
  year={2025},
  url={https://arxiv.org/pdf/2406.15142.pdf}
}

@article{Yang2025SIM-RAG,
  title={SIM-RAG: Knowing You Don’t Know: Learning When to Continue Search in Multi-round Retrieval-Augmented Generation through Self-Practicing},
  author={Yang, Jiecao and others},
  journal={arXiv preprint arXiv:2405.14074},
  year={2025},
  url={https://arxiv.org/pdf/2405.14074.pdf}
}

@article{Fang2024RAAT,
  title={RAAT: Enhancing Noise Robustness of Retrieval-Augmented Language Models with Adaptive Adversarial Training},
  author={Fang, Hao and Wu, Lijun and Wang, Yongjie and others},
  journal={ICLR 2024},
  year={2024},
  url={https://arxiv.org/pdf/2310.05734.pdf}
}

@article{Hwang2025RARAG,
  title={Retrieval-Augmented Generation with Estimation of Source Reliability},
  author={Hwang, Jeongyeon and Park, Junyoung and Park, Hyejin and Kim, Dongwoo and Park, Sangdon and Ok, Jungseul},
  journal={arXiv preprint arXiv:2410.22954},
  year={2025},
  url={https://arxiv.org/pdf/2410.22954.pdf}
}

@article{Percin2025RobustnessQuery,
  title={Investigating the Robustness of Retrieval-Augmented Generation at the Query Level},
  author={Perçin, Sezen and others},
  journal={arXiv preprint arXiv:2507.06956},
  year={2025},
  url={https://arxiv.org/pdf/2507.06956.pdf}
}

@article{Tu2025RbFT,
  title={RbFT: Robust Fine-Tuning for Retrieval-Augmented Generation against Retrieval Defects},
  author={Tu, Yuchen and others},
  journal={AAAI 2025},
  year={2025},
  url={https://arxiv.org/pdf/2504.02273.pdf}
}

@article{Drushchak2025mRAG,
  title={Multimodal Retrieval-Augmented Generation: Unified Information Processing Across Text, Image, Table, and Video Modalities},
  author={Drushchak, Nazarii and Polyakovska, Nataliya and Bautina, Maryna and others},
  journal={Proceedings of the 1st Workshop on Multimodal Augmented Generation via Multimodal Retrieval (MAGMaR 2025)},
  year={2025},
  url={https://arxiv.org/pdf/2503.18016.pdf}
}

@article{Han2025GraphRAG,
  title={Retrieval-Augmented Generation with Graphs (GraphRAG)},
  author={Han, Haoyu and Wang, Yu and Shomer, Harry and others},
  journal={arXiv preprint arXiv:2501.00309},
  year={2025},
  url={https://arxiv.org/pdf/2501.00309.pdf}
}

@article{Cheng2025RemoteRAG,
  title={RemoteRAG: A Privacy-Preserving LLM Cloud RAG Service},
  author={Cheng, Yihang and Zhang, Lan and Wang, Junyang and Yuan, Mu and Yao, Yunhao},
  journal={Findings of the Association for Computational Linguistics: ACL 2025},
  year={2025},
  url={https://aclanthology.org/2025.findings-acl.197.pdf}
}

@article{Chen2025AQUA,
  title={Safeguarding Multimodal Knowledge Copyright in the RAG-as-a-Service Environment},
  author={Chen, Tianyu and Lou, Jian and Wang, Wenjie},
  journal={arXiv preprint arXiv:2506.10030},
  year={2025},
  url={https://arxiv.org/pdf/2506.10030.pdf}
}

@article{Ammann2025SecuringRAG,
  title={Securing RAG: A Risk Assessment and Mitigation Framework},
  author={Ammann, Lukas and Ott, Sara and Landolt, Christoph R. and Lehmann, Marco P.},
  journal={arXiv preprint arXiv:2505.08728},
  year={2025},
  url={https://arxiv.org/pdf/2505.08728.pdf}
}

@article{Tyndall2025OfflineRAG,
  title={Feasibility Evaluation of Secure Offline Large Language Models with Retrieval-Augmented Generation for CPU-Only Inference},
  author={Tyndall, Erick and Wagner, Torrey and Gayheart, Colleen and Some, Alexandre and Langhals, Brent},
  journal={Information},
  year={2025},
  url={https://www.mdpi.com/2078-2489/16/9/744}
}

@article{Gan2025RAGEvaluationSurvey,
  title={Retrieval Augmented Generation Evaluation in the Era of Large Language Models: A Comprehensive Survey},
  author={Gan, Aoran and Yu, Hao and Zhang, Kai and Liu, Qi and Yan, Wenyu and Huang, Zhenya and Tong, Shiwei and Chen, Enhong and Hu, Guoping},
  journal={Frontiers of Computer Science},
  year={2025},
  note={in press},
  url={https://arxiv.org/pdf/2504.14891.pdf}
}

@article{Yu2024RAGEvalSurvey,
  title={Evaluation of Retrieval-Augmented Generation: A Survey},
  author={Yu, Hao and Gan, Aoran and Zhang, Kai and Tong, Shiwei and Liu, Qi and Liu, Zhaofeng},
  journal={arXiv preprint arXiv:2405.07437},
  year={2024},
  url={https://arxiv.org/pdf/2405.07437.pdf}
}

@article{Li2024LCvsRAG,
  title={Retrieval Augmented Generation or Long-Context LLMs? A Comprehensive Study and Hybrid Approach},
  author={Li, Zhuowan and Li, Cheng and Zhang, Mingyang and Mei, Qiaozhu and Bendersky, Michael},
  journal={Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing: Industry Track},
  year={2024},
  url={https://arxiv.org/pdf/2404.13604.pdf}
}

@article{Xu2024SimRAGDomain,
  title={SimRAG: Self-Improving Retrieval-Augmented Generation for Domain Adaptation},
  author={Xu, Zihan and others},
  journal={arXiv preprint arXiv:2402.00541},
  year={2024},
  url={https://arxiv.org/pdf/2402.00541.pdf}
}

@article{Liu2025RoseRAG,
  title={RoseRAG: Robust Retrieval-Augmented Generation with Small-scale LLMs via Margin-aware Preference Optimization},
  author={Liu, Tianci and others},
  journal={Findings of ACL 2025},
  year={2025},
  url={https://aclanthology.org/2025.findings-acl.676.pdf}
}

```

### `chapters/Cap_1.tex`

```tex
\chapter{Introducción}

En los últimos años, los Modelos de Lenguaje Grandes (LLMs, por sus siglas en inglés) han revolucionado el campo del procesamiento del lenguaje natural, demostrando capacidades sin precedentes para comprender y generar texto de manera coherente y contextualizada. Sin embargo, su eficacia está inherentemente limitada por el conocimiento estático adquirido durante su entrenamiento, lo que conduce a problemas bien documentados como las ``alucinaciones'' —respuestas plausibles pero incorrectas— y la incapacidad para acceder a información actualizada o a dominios de conocimiento privados.

Para mitigar estas limitaciones, ha surgido con fuerza el paradigma de Generación Aumentada por Recuperación (RAG, por sus siglas en inglés). RAG mejora las capacidades de los LLMs al permitirles acceder a fuentes de conocimiento externas en tiempo real, recuperando documentos o fragmentos de datos relevantes para una consulta específica y utilizándolos como contexto adicional para fundamentar sus respuestas \cite{Gan2025RAGEvaluationSurvey}. Este enfoque no solo aumenta la fiabilidad y precisión de los modelos, sino que también abre la puerta a su aplicación en entornos empresariales que manejan datos privados y dinámicos.

La creciente adopción de esta tecnología ha dado lugar al modelo de RAG-como-Servicio (RAGaaS), donde las soluciones RAG se alojan en la nube, facilitando su acceso y escalabilidad. No obstante, este modelo de despliegue introduce importantes desafíos en materia de privacidad y seguridad, ya que las consultas de los usuarios, que pueden contener información sensible, deben ser transmitidas a servidores de terceros para su procesamiento \cite{Cheng2025RemoteRAG}. Esta exposición de datos resulta inaceptable en numerosos sectores, como el gubernamental, el financiero o el sanitario, donde la confidencialidad y la soberanía de los datos son requisitos indispensables.

Es en este contexto donde surge la necesidad de soluciones RAG que puedan operar de manera segura en entornos completamente desconectados (offline), garantizando que los datos nunca abandonen la infraestructura local de una organización. Este trabajo de tesis propone y desarrolla Atenex, un framework de RAG diseñado específicamente para abordar esta brecha, ofreciendo una solución robusta, privada y eficiente para despliegues offline.

\section{Motivación y Contexto}

La evolución de los LLMs ha sido exponencial, pero sus limitaciones intrínsecas han impulsado a la comunidad científica a explorar arquitecturas que extiendan sus capacidades. La Generación Aumentada por Recuperación (RAG) se ha consolidado como la solución predominante, al integrar un recuperador de información que provee evidencia externa para que el LLM genere respuestas más fiables y actualizadas \cite{Yu2024RAGEvalSurvey}. La investigación en RAG ha avanzado rápidamente, explorando desde la optimización de los recuperadores y la fusión de resultados hasta la aplicación en dominios multimodales que incluyen texto, imágenes y video \cite{Drushchak2025mRAG, Zheng2025RAGinVision}.

Sin embargo, el paradigma RAGaaS, si bien democratiza el acceso a esta tecnología, plantea serias preocupaciones de privacidad. El trabajo de \cite{Cheng2025RemoteRAG} demuestra formalmente el riesgo de fuga de información semántica a través de las consultas y los embeddings enviados a la nube, proponiendo como solución mecanismos de privacidad diferencial que, aunque efectivos, aún se encuentran en fase de investigación.

Paralelamente, existe un creciente debate sobre la necesidad de RAG en la era de los LLMs con ventanas de contexto cada vez más largas (LC-LLMs). Estudios comparativos como el de \cite{Li2024LCvsRAG} concluyen que, si bien los LC-LLMs pueden procesar grandes volúmenes de texto, RAG sigue siendo superior en términos de eficiencia, costo computacional y precisión, ya que pre-filtra y enfoca al modelo únicamente en la información más relevante. Esta conclusión refuerza la vigencia de RAG, especialmente en entornos con recursos computacionales limitados.

Finalmente, la viabilidad de desplegar sistemas de IA complejos en hardware local y sin aceleración por GPU es un área de investigación crítica pero poco explorada. El trabajo de \cite{Tyndall2025OfflineRAG} evalúa el rendimiento de LLMs con RAG en sistemas CPU-only, concluyendo que tales despliegues son factibles para tareas estructuradas como la respuesta a preguntas factuales, aunque con limitaciones en tareas generativas más complejas como la sumarización. Este hallazgo proporciona una base empírica fundamental que motiva el desarrollo de frameworks optimizados para entornos offline como Atenex.

\section{Planteamiento del Problema}

A pesar de los beneficios demostrados de RAG, su adopción en entornos corporativos y gubernamentales con altos requisitos de seguridad se ve obstaculizada por una brecha fundamental: la dependencia de servicios en la nube y la consiguiente exposición de datos sensibles. Las soluciones RAGaaS exponen las consultas de los usuarios a proveedores externos, creando un riesgo de privacidad inaceptable para muchas organizaciones \cite{Cheng2025RemoteRAG}. Por otro lado, la implementación de un sistema RAG local, seguro, eficiente y robusto desde cero representa un desafío técnico considerable, ya que requiere la integración de múltiples componentes complejos, desde bases de datos vectoriales y recuperadores híbridos hasta la gestión segura de múltiples usuarios (multi-tenant) con aislamiento de datos.

Actualmente, no existe en el ecosistema de código abierto un framework integral que ofrezca una solución RAG offline, orientada a la privacidad y lista para su despliegue en entornos multi-tenant, que sea a la vez robusta frente a información ruidosa y computacionalmente eficiente.

La pregunta central de investigación de esta tesis es: \textbf{¿cómo se puede diseñar e implementar un framework de Generación Aumentada por Recuperación (Atenex) que opere de manera segura y eficiente en un entorno offline y multi-tenant, garantizando la privacidad de las consultas y la robustez de las respuestas, para ofrecer una alternativa viable a las soluciones RAG dependientes de la nube?}

\section{Objetivos}

El objetivo general de esta tesis es diseñar, implementar y evaluar Atenex, un framework de Generación Aumentada por Recuperación (RAG) de código abierto, enfocado en la privacidad y la seguridad para su despliegue en entornos offline y multi-tenant.

\subsection{Objetivos Específicos}

Para alcanzar el objetivo general, se plantean los siguientes objetivos específicos:

\begin{enumerate}
    \item Analizar el estado del arte de las arquitecturas RAG y RAGaaS, identificando las brechas existentes en materia de privacidad, seguridad y robustez en despliegues offline.
    \item Diseñar e implementar la arquitectura modular de Atenex, incorporando un pipeline de recuperación híbrida, mecanismos de re-ranking (reranking) y una gestión multi-tenant segura para el aislamiento de datos entre usuarios.
    \item Desarrollar y evaluar un mecanismo de ofuscación de consultas basado en la perturbación de embeddings para proteger la privacidad del usuario en el framework Atenex, midiendo el balance entre el nivel de privacidad y la precisión de la recuperación de información.
    \item Validar la robustez del sistema Atenex frente a evidencia conflictiva o ruidosa mediante la creación de un benchmark de evaluación, y proponer un mecanismo de filtrado para mitigar el impacto de la desinformación en las respuestas generadas.
    \item Realizar un análisis comparativo de rendimiento (latencia, precisión y costo computacional) entre Atenex y el enfoque de usar Modelos de Lenguaje de Gran Contexto (LC-LLMs) en tareas de pregunta-respuesta sobre un corpus documental especializado.
\end{enumerate}

\section{Organización de la tesis}

La presente tesis se estructura de la siguiente manera para abordar los objetivos planteados:

\begin{itemize}
    \item \textbf{Capítulo 1: Introducción.} Se presenta la motivación, el contexto del problema, los objetivos de la investigación y la organización general del documento.
    \item \textbf{Capítulo 2: Marco Teórico.} Se revisan los conceptos fundamentales de los Modelos de Lenguaje Grandes (LLMs), las arquitecturas de Generación Aumentada por Recuperación (RAG), las técnicas de recuperación de información, y se profundiza en los desafíos de privacidad, seguridad y robustez en dichos sistemas.
    \item \textbf{Capítulo 3: Propuesta: Atenex.} Se describe en detalle la arquitectura del framework Atenex, detallando cada uno de sus componentes, el pipeline de procesamiento de datos, el flujo de recuperación de información y el diseño del módulo de privacidad.
    \item \textbf{Capítulo 4: Pruebas y Resultados.} Se presentan la metodología experimental, los conjuntos de datos utilizados y los resultados obtenidos al evaluar la privacidad, robustez y eficiencia de Atenex. Se incluye el análisis comparativo con otros enfoques de vanguardia.
    \item \textbf{Capítulo 5: Conclusiones y Trabajos Futuros.} Se resumen los hallazgos principales de la investigación, se discuten las limitaciones del trabajo realizado y se proponen líneas de investigación futuras para extender y mejorar la propuesta.
\end{itemize}

```

### `chapters/Cap_2.tex`

```tex
\chapter{Marco Teórico}
\label{chap:background}

\noindent
En este capítulo se presenta el marco teórico que sustenta el desarrollo de la tesis. Se revisan los orígenes y la evolución de la Generación Aumentada por Recuperación (RAG), su relación con los modelos de lenguaje grandes (LLMs) y su consolidación como paradigma para integrar datos externos en las tareas de generación. Además, se abordan las variantes más recientes, como las implementaciones como servicio (RAGaaS) y las arquitecturas offline, así como los desafíos de privacidad, robustez y evaluación que caracterizan al estado del arte.

\section{Modelos de lenguaje y limitaciones}

Los modelos de lenguaje grandes han revolucionado el procesamiento del lenguaje natural gracias a su capacidad para modelar distribuciones complejas de texto. Sin embargo, al depender exclusivamente de conocimiento paramétrico, los LLMs sufren de alucinaciones y carecen de mecanismos para incorporar información nueva o privada. Esta limitación motivó el desarrollo de técnicas que integran memoria no paramétrica, como las arquitecturas de búsqueda y recuperación \cite{Lewis2020RAG,Guu2020REALM}.

\section{Generación Aumentada por Recuperación (RAG)}

\subsection{Definición y arquitectura básica}

La Generación Aumentada por Recuperación combina un componente de \emph{retrieval} (recuperación) con un modelo generativo. El sistema indexa una colección de documentos en un espacio vectorial, formula una consulta a partir del \emph{prompt} del usuario y recupera los fragmentos más relevantes. Estos fragmentos se concatenan al \emph{prompt} y sirven de contexto adicional para que el LLM genere respuestas fundamentadas. La arquitectura original de RAG \cite{Lewis2020RAG} demostró que este enfoque reduce las alucinaciones y permite actualizar el conocimiento sin reentrenar el modelo.

\subsection{Variantes y mejoras recientes}

Desde su introducción, RAG ha evolucionado en varias líneas de investigación:

\begin{itemize}
  \item \textbf{Mejoras en el recuperador:} optimización de la búsqueda mediante reescritura de consultas (RQ‑RAG) \cite{Chan2024RQ-RAG}, recuperación jerárquica (LevelRAG) \cite{Zhang2025LevelRAG} o exploración multi‑salto (HopRAG) \cite{Wang2025HopRAG}. Otras propuestas incorporan reordenamiento y filtrado de contexto para descartar fragmentos irrelevantes \cite{Wang2023FILCO}.
  \item \textbf{Mejoras en el generador:} bucles de auto‑reflexión donde el modelo critica y corrige sus respuestas (Self‑RAG) \cite{Asai2023SelfRAG}, memorias internas para reutilizar salidas previas (SelfMem) \cite{Cheng2023SelfMem} y compresión de contexto para preguntas multi‑hop (BRIEF) \cite{Li2025BRIEF}.
  \item \textbf{Arquitecturas híbridas:} modelos como IM‑RAG \cite{Yang2024IM-RAG}, CoRAG \cite{Wang2025CoRAG} y SIM‑RAG \cite{Yang2025SIM-RAG} realizan múltiples rondas de recuperación y generación, permitiendo consultas de seguimiento y decisiones dinámicas sobre cuándo detener la búsqueda.
  \item \textbf{Robustez y seguridad:} RAAT aplica entrenamiento adversarial para manejar ruido en la recuperación \cite{Fang2024RAAT}. RA‑RAG estima la fiabilidad de las fuentes y combina documentos mediante votación ponderada \cite{Hwang2025RARAG}. Otros trabajos analizan la sensibilidad del sistema a perturbaciones en las consultas \cite{Percin2025RobustnessQuery} y proponen fine‑tuning robusto frente a defectos de recuperación (RbFT) \cite{Tu2025RbFT}.
  \item \textbf{RAG multimodal y con grafos:} la integración de imágenes, tablas y videos se aborda en mRAG \cite{Drushchak2025mRAG}, mientras que GraphRAG explora grafos de conocimiento para representar relaciones complejas \cite{Han2025GraphRAG}.
\end{itemize}

\section{RAG como Servicio (RAGaaS)}

El modelo RAG‑como‑Servicio ofrece pipelines de recuperación y generación alojados en la nube. Este enfoque democratiza la adopción, pero introduce desafíos de privacidad: las consultas pueden contener información sensible y el proveedor del servicio controla la base de conocimiento. RemoteRAG formaliza el servicio RAG con preservación de privacidad y propone perturbar los embeddings de las consultas mediante un esquema de privacidad diferencial \cite{Cheng2025RemoteRAG}. Otro aspecto crítico es la protección de los derechos de autor en bases de conocimiento compartidas; AQUA inserta marcas de agua semánticas en imágenes para rastrear su uso indebido \cite{Chen2025AQUA}. Finalmente, el marco \emph{Securing RAG} analiza los principales vectores de ataque (inversión de embeddings, inyección de prompts, contaminación de datos) y ofrece directrices para mitigar riesgos en despliegues de RAGaaS \cite{Ammann2025SecuringRAG}.

\section{RAG offline y despliegues locales}

Para sectores que exigen confidencialidad absoluta y control de los datos, se requieren implementaciones de RAG completamente locales. Estos sistemas combinan LLMs de código abierto desplegados en servidores o computadoras personales con índices vectoriales locales (FAISS, ChromaDB) que almacenan los embeddings de los documentos. La evaluación de \cite{Tyndall2025OfflineRAG} muestra que ejecutar RAG en hardware solo CPU es factible para preguntas factuales, aunque las tareas de sumarización presentan mayor latencia y variabilidad. Guías prácticas destacan la importancia de cuantizar los modelos y optimizar recursos para mantener el rendimiento.

\section{Evaluación y métricas}

Evaluar un sistema RAG requiere medir tanto la relevancia de la recuperación como la fidelidad de la generación. Las encuestas recientes proponen métricas que combinan exactitud de recuperación con exactitud factual y trazabilidad de las respuestas \cite{Gan2025RAGEvaluationSurvey,Yu2024RAGEvalSurvey}. Herramientas como RAGAS permiten estimar qué parte de la respuesta está sustentada en la evidencia recuperada. Otros estudios comparan RAG con LLMs de contexto largo y muestran que RAG sigue siendo más eficiente y preciso al filtrar información irrelevante \cite{Li2024LCvsRAG}.

\section{Estado del arte y tendencias}

Las tendencias actuales incluyen la recuperación jerárquica y multi‑hop \cite{Zhang2025LevelRAG,Wang2025HopRAG}, la compresión de contexto \cite{Li2025BRIEF}, la adaptación al dominio mediante auto‑entrenamiento \cite{Xu2024SimRAGDomain}, la robustez para modelos pequeños \cite{Liu2025RoseRAG} y el uso de grafos para razonamiento estructurado \cite{Han2025GraphRAG}. Asimismo, la privacidad y la seguridad se han convertido en ejes transversales, con propuestas como RemoteRAG y AQUA para proteger consultas y datos \cite{Cheng2025RemoteRAG,Chen2025AQUA}.

\section{Consideraciones finales}

La investigación en RAG ha avanzado rápidamente, proponiendo soluciones para mejorar la recuperación, la generación y la robustez del sistema. Sin embargo, subsisten retos relacionados con la seguridad, la privacidad y la adaptación a diferentes dominios. Las variantes offline y RAGaaS representan extremos complementarios que ilustran el compromiso entre confidencialidad y escalabilidad. El próximo capítulo presenta la propuesta \emph{Atenex}, un framework que busca conciliar estos requerimientos mediante una arquitectura modular, mecanismos de protección de consultas y gestión multi‑usuario.

```

### `chapters/Cap_3.tex`

```tex
\chapter{Propuesta}\label{chap:proposal}

En este capítulo se desarrolla toda la propuesta realizada a través de la investigación. Sigue la misma estructura del capítulo anterior.

El título del capítulo es flexible de acuerdo a cada tesis. Algunos títulos sugeridos podrían ser:

\begin{itemize}
\item Algoritmo X: nuestra propuesta.
\item Modelo MRLO
\end{itemize}

\section{Sección 1 del Capítulo II}

Un capítulo puede contener $n$ secciones. 

Con respecto a las referencias bibliográficas, se hace de la siguiente manera \cite{Mateos00}, se debe de redactar el texto de forma tal que si se retiraran las referencias, la redacción no debe perjudicarse. Por ejemplo 

\subsubsection{Localización y Mapeo Simultaneos SLAM puramente topológico}
Se puede hacer un sistema SLAM con sólo reconocimiento de lugares, generando un SLAM topológico con representación basada en grafos \cite{Choset_2001}. Se almacena un registro de lugares visitados y cómo se llega de uno a otro (conexiones) sin tener información geométrica explícita.

Este tipo de SLAM es más adecuado para planificación y navegación. La figura \ref{Fig:SLAMTopological} ilustra este tipo de SLAM 
	
	\begin{figure}[ht]
		\centering
			\includegraphics[width = 6cm]{figs/Chap2/SLAM-TopologicalRepresentation.jpg}
			\label{Fig:SLAMTopological}
			\caption{SLAM puramente topológico basado en grafos}
	\end{figure}


\subsection{Sub Sección}

Una sección puede contener $n$ sub secciones.\cite{Galante01}

\subsubsection{Sub sub sección}

Una sub sección puede contener n sub sub secciones.

\section{Recomendaciones generales de escritura}
Un trabajo de esta naturaleza debe tener en consideración varios aspectos generales:

\begin{itemize}
\item Ir de lo genérico a lo específico. Siempre hay qeu considerar que el lector podría ser alguien no muy familiar con el tema 
y la lectura debe serle atractiva.
\item No redactar frases muy largas. Si las frases tienen más de 2 líneas continuas es probable que la lectura sea dificultosa.
\item Las figuras, ecuaciones, tablas deben ser citados y explicados {\bf antes} de que aparezcan en el documento, como la Tabla \ref{tab:OSProcesses} que se presenta a continuación.

\begin{table}[ht]
	\centering
	{\footnotesize
    \begin{tabular}{p{0.20\textwidth}@{\;}p{0.65\textwidth}}
		\toprule
    \multicolumn{1}{c}{\bf Motivo} & \multicolumn{1}{c}{\bf Explicación} \\
    \midrule
    {\bf \textit{Swaping}} & El SO requiere liberar memoria principal para cederla a un proceso listo para ejecución. \\
{\bf Otras razones SO} & El SO puede suspender un proceso en segundo plano o una utilidad o un proceso que parece causar problemas. \\
{\bf Pedido del usuario} &  Un usuario puede solicitar suspensión por {\it debugging} o espera de conexión para uso de un recurso. \\
{\bf Temporización} &  Un proceso que se ejecuta periódicamente que puede esperar su proxima temporización en modo suspendido. \\
{\bf A pedido del proceso padre} &  Un proceso padre puede suspender la ejecución de sus procesos hijos para analizarlos o gestionarlos.\\
\bottomrule
\end{tabular}
}
	\caption{Ejemplo de tabla Procesos en Sistemas Operativos}
	\label{tab:OSProcesses}
\end{table}


\item Encadenar las ideas. Ninguna frase debe estar suelta. Siempre que terminas un párrafo y hay otro a continuación, 
el primero debe dejar abierta la idea que se explicará a continuación. Todo debe tener secuencia.
\end{itemize}


\section{Consideraciones Finales}

Cada capítulo excepto el primero debe contener, al finalizar, una sección de consideraciones que enlacen el presente capítulo con el siguiente.

```

### `chapters/Cap_4.tex`

```tex
\chapter{Pruebas y Resultados}

```

### `chapters/conclusiones.tex`

```tex
\chapter{Conclusiones y Trabajos Futuros}\label{chap:conclusiones}

Las conclusiones de la tesis son una parte muy importante y tiene las siguientes partes.

Escribir las conclusiones de su trabajo por cada uno de los objetivos específicos definidos en el Capítulo 1 y luego escribir la conclusión general de tu trabajo.

\section{Problemas encontrados}
La segunda  parte de este capítulo corresponde a los problemas encontrados, esta sección es muy importante para que los siguientes estudiantes o investigadores que hagan algo en esta línea no cometan los mismos errores y tu tesis sea un buen peldaño para avanzar más rápido.

\section{Recomendaciones}
En esta sección el tesista debe reflejar que la tesis ha permitido adquirir nuevos conocimientos que podrían servir para guiar otros trabajos en el futuro.

\section{Trabajos futuros}
En base a los puntos anteriores es recomendable que tu tesis también sugiera trabajos futuros. Esta sección es esecialmente útil para otras ideas de tesis.

Todo este capítulo no debe ser más de unas 4 páginas.

```

### `export_codebase.py`

```py
from pathlib import Path

# Carpetas que queremos excluir al recorrer la codebase
EXCLUDED_DIRS = {'.git', '__pycache__', '.venv', '.idea', '.mypy_cache', '.vscode', '.github', 'node_modules'}

def build_tree(directory: Path, prefix: str = "") -> list:
    """
    Genera una representación en árbol de la estructura de directorios y archivos,
    excluyendo las carpetas especificadas en EXCLUDED_DIRS.
    """
    # Filtrar y ordenar los elementos del directorio
    entries = sorted(
        [entry for entry in directory.iterdir() if entry.name not in EXCLUDED_DIRS],
        key=lambda e: e.name
    )
    tree_lines = []
    for index, entry in enumerate(entries):
        connector = "└── " if index == len(entries) - 1 else "├── "
        tree_lines.append(prefix + connector + entry.name)
        if entry.is_dir():
            extension = "    " if index == len(entries) - 1 else "│   "
            tree_lines.extend(build_tree(entry, prefix + extension))
    return tree_lines

def generate_codebase_markdown(base_path: str = ".", output_file: str = "full_codebase.md"):
    base = Path(base_path).resolve()

    # Si existe un paquete con el nombre del proyecto, preferirlo
    candidates = [p for p in base.iterdir() if p.is_dir() and p.name not in EXCLUDED_DIRS]
    preferred = None
    for c in candidates:
        if c.name == "atenex_offline":
            preferred = c
            break

    # Fallback: usar la carpeta preferida si existe, o la raíz del repo
    if preferred:
        app_dir = preferred
    else:
        # Si no hay una carpeta específica, usar la raíz (base)
        app_dir = base

    lines = []

    # Agregar la estructura de directorios al inicio del Markdown
    lines.append("# Estructura de la Codebase")
    lines.append("")
    lines.append("```")
    lines.append(f"{app_dir.name}/")
    tree_lines = build_tree(app_dir)
    lines.extend(tree_lines)
    lines.append("```")
    lines.append("")

    # Agregar el contenido de la codebase en Markdown
    lines.append(f"# Codebase: `{app_dir.name}`")
    lines.append("")

    # Recorrer solo la carpeta app
    for path in sorted(app_dir.rglob("*")):
        # Ignorar directorios excluidos
        if any(part in EXCLUDED_DIRS for part in path.parts):
            continue

        if path.is_file():
            rel_path = path.relative_to(base)
            lines.append(f"## File: `{rel_path}`")
            try:
                content = path.read_text(encoding='utf-8')
            except UnicodeDecodeError:
                lines.append("_[Skipped: binary or non-UTF8 file]_")
                continue
            except Exception as e:
                lines.append(f"_[Error al leer el archivo: {e}]_")
                continue
            ext = path.suffix.lstrip('.')
            lang = ext if ext else ""
            lines.append(f"```{lang}")
            lines.append(content)
            lines.append("```")
            lines.append("")

    # Agregar pyproject.toml si existe en la raíz
    toml_path = base / "pyproject.toml"
    if toml_path.exists():
        lines.append("## File: `pyproject.toml`")
        try:
            content = toml_path.read_text(encoding='utf-8')
        except UnicodeDecodeError:
            lines.append("_[Skipped: binary or non-UTF8 file]_")
        except Exception as e:
            lines.append(f"_[Error al leer el archivo: {e}]_")
        else:
            lines.append("```toml")
            lines.append(content)
            lines.append("```")
            lines.append("")

    output_path = base / output_file
    try:
        output_path.write_text("\n".join(lines), encoding='utf-8')
        print(f"[OK] Código exportado a Markdown en: {output_path}")
    except Exception as e:
        print(f"[ERROR] Error al escribir el archivo de salida: {e}")

# Si se corre el script directamente
if __name__ == "__main__":
    generate_codebase_markdown()

```

### `figs/Chap2/SLAM-TopologicalRepresentation.jpg`

```jpg
[binary file omitted – 85717 bytes]

```

### `figs/UCSP_black.pdf`

```pdf
[binary file omitted – 17870 bytes]

```

### `frontmatter/_README.txt`

```txt
This folder contains front-matter TeX files: abreviaturas.tex, Abstract.tex, Agradecimientos.tex, Resumen.tex

Do not edit the originals in project root until main file updated. Update `Tesis.tex` includes to point here.

```

### `frontmatter/abreviaturas.aux`

```aux
\relax 
\newacro{SPC}[\AC@hyperlink{SPC}{SPC}]{Sociedad Peruana de Computación}
\newacro{CMM}[\AC@hyperlink{CMM}{CMM}]{\textit  {Capability Maturity Model}}
\@setckpt{frontmatter/abreviaturas}{
\setcounter{page}{4}
\setcounter{equation}{0}
\setcounter{enumi}{0}
\setcounter{enumii}{0}
\setcounter{enumiii}{0}
\setcounter{enumiv}{0}
\setcounter{footnote}{0}
\setcounter{mpfootnote}{0}
\setcounter{part}{0}
\setcounter{chapter}{0}
\setcounter{section}{0}
\setcounter{subsection}{0}
\setcounter{subsubsection}{0}
\setcounter{paragraph}{0}
\setcounter{subparagraph}{0}
\setcounter{figure}{0}
\setcounter{table}{0}
}

```

### `frontmatter/abreviaturas.tex`

```tex
%mayores detalles de como usas las abreviaturas (acronimos)
% vea: http://www.ctan.org/tex-archive/macros/latex/contrib/acronym/
% hay un manual en pdf en esa misma direccion

\chapter*{Abreviaturas}

\begin{acronym}
\acro{SPC}{Sociedad Peruana de Computación}
\acro{CMM}{\textit{Capability Maturity Model}}
\end{acronym}

```

### `frontmatter/Abstract.tex`

```tex
\begin{abstract}
Here you should enter up to 300 words maximum, describing the problem you are trying to solve, the justification, contributions or solutions you are proposing, how much you have achieved them in terms of obtained outcomes.
\end{abstract}

```

### `frontmatter/Agradecimientos.tex`

```tex
\begin{agradecimientos}
Aquí deberás colocar a quien y porque agradeces. Ejemplo:

En primer lugar deseo agradecer a Dios por haberme guiado a lo largo de estos cinco años de estudio.

Agradezco a mis padres por el apoyo brindado para forjarme como un profesional.

Agradezco a la universidad, mi \textit{alma matter}, por haberme cobijado y brindado la formación que ahora me permitirá ayudar a construir una mejor sociedad.

Agradezco de forma muy especial a mi asesor Prof. Dr./Mag. nombre 1 por haberme guiado en esta tesis. ...

Deseo agradecer de forma especial a mis profesores: nombre 1, nombre 2, nombre 3 porque fueron ejemplos que deseo seguir en mi vida profesional.

Deseo agradecer al personal administrativo de la universidad: nombre 1, nombre 2, nombre 3. Muchas gracias por la atención brindada y porque siempre estuvieron dispuestas a ayudarnos.
\end{agradecimientos}

```

### `frontmatter/Resumen.tex`

```tex
\begin{resumen}
Aquí deberás colocar hasta 300 palabras como máximo, describiendo el problema que intentas resolver, la justificación, aportes o soluciones que planteas, qué tanto los haz alcanzado en calidad de resultados obtenidos.
\end{resumen}

```

### `scripts/fix_quotes.py`

```py
"""Normalize quotation marks in .tex files to LaTeX conventions.

This script finds straight ASCII double quotes and common Unicode “smart”
quotes and replaces them with LaTeX-style opening `` and closing '' marks.
It operates on .tex files under the given root (default: repository root).
Backups are written with a .bak extension next to each modified file.

Usage: python scripts/fix_quotes.py [--root ROOT]
"""
from pathlib import Path
import re
import argparse


REPLACEMENTS = [
    # Common Unicode smart quotes to ASCII double quote
    (re.compile(r'[\u201C\u201D\u201E\u201F]'), '"'),
    # Left single smart quote and right single smart quote -> ASCII '
    (re.compile(r"[\u2018\u2019\u201A\u201B]"), "'")
]

DOUBLE_QUOTE_PATTERN = re.compile(r'"(.*?)"', flags=re.DOTALL)


def normalize_text(text: str) -> str:
    # First normalize smart quotes to ASCII equivalents
    for pat, repl in REPLACEMENTS:
        text = pat.sub(repl, text)

    # Then convert ASCII double-quoted segments to LaTeX quotes
    def repl(m: re.Match) -> str:
        inner = m.group(1)
        return '``' + inner + "''"

    return DOUBLE_QUOTE_PATTERN.sub(repl, text)


def process_file(path: Path) -> bool:
    text = path.read_text(encoding='utf-8')
    new = normalize_text(text)
    if new != text:
        bak = path.with_suffix(path.suffix + '.bak')
        bak.write_text(text, encoding='utf-8')
        path.write_text(new, encoding='utf-8')
        return True
    return False


def find_tex_files(root: Path):
    for p in root.rglob('*.tex'):
        yield p


def main():
    parser = argparse.ArgumentParser()
    parser.add_argument('--root', default='.', help='Repository root path')
    args = parser.parse_args()
    root = Path(args.root).resolve()

    modified = []
    for f in find_tex_files(root):
        try:
            if process_file(f):
                modified.append(str(f.relative_to(root)))
        except Exception as e:
            print(f"Error processing {f}: {e}")

    if modified:
        print('Modified files:')
        for m in modified:
            print(' -', m)
    else:
        print('No changes made.')


if __name__ == '__main__':
    main()

```

### `Tesis.tex`

```tex
% Template LaTeX Tesis Ciencia de la Computación UCSP
% 2022

\documentclass[a4paper,openany,12pt]{book}
\usepackage[utf8]{inputenc}
\usepackage{graphicx}
\usepackage[spanish,mexico]{babel}
% custom style files are in the project's sty/ folder
\usepackage{sty/fancyhdr}
\usepackage{ae}
\usepackage[left=2.5cm,right=2.5cm,top=3cm,bottom=2cm]{geometry}
\usepackage[printonlyused]{acronym}
\usepackage{xspace}
\usepackage{sty/hlundef}
\usepackage{sty/tesis}
\usepackage{setspace}
\usepackage{booktabs}
% removed an empty \usepackage{} which caused a LaTeX error

\title{** Título de la Tesis **}
\title{Una Estructura de Datos para la Consulta Visual Interactiva por Similitud de Grandes Volúmenes de Datos Multidimensionales Mixtos Georeferenciados}

\author{** Nombre completo del tesista **}


\advisor{** Dr./Mg. Nombre completo del Asesor **}


\date{** Arequipa, Mes Año **}

%\examinerone{Prof. Dr. Hidalgo Buena Gente}{Presidente}%
%\examinertwo{Prof. Dr. Antero A. Gal Oppe}{Secretario}%
%\examinerthree{Prof. Dr. Liu Xiao Ling}{University of ...} % of being the case

\dedicado{Aquí deberás colocar a quien va dedicada tu tesis por ejemplo: A Dios, por todo lo que me ha dado, a todos los profesores por sus enseñanzas y algunos amigos.}

\begin{document}
\pagestyle{fancy}

\maketitle %Compone la carátula y la dedicatoria
\newpage

%\approved{\tres}%  {\tres} or {\cuatro}

% Front matter is located in frontmatter/ folder
% use \input so aux files are written alongside the main document
\input{frontmatter/abreviaturas}

\input{frontmatter/Agradecimientos} %Inserta los agradecimientos
\input{frontmatter/Resumen} %Inserta el resumen
\input{frontmatter/Abstract} %Inserta el abstract

\pagenumbering{roman}
\setcounter{page}{1}
\pagestyle{plain}

\tableofcontents %Inserta el índice general
\listoftables %Inserta el índice de cuadros
\listoffigures %Inserta el índice de figuras

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%   En esta parte deberas incluir los archivos de tu tesis   %%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\pagestyle{plain}
\pagenumbering{arabic}
\setcounter{page}{1}
% chapters are in the chapters/ folder
\input{chapters/Cap_1} %Inserta el capítulo 1
\input{chapters/Cap_2} %Inserta el capítulo 2
\input{chapters/Cap_3} %Inserta el capítulo 3
\input{chapters/Cap_4} %Inserta el capítulo 4
\input{chapters/conclusiones} %Inserta el capítulo 5

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\bibliographystyle{apalike}

% bibliography file is in the bibliography/ directory
\bibliography{bibliography/Bibliog}
\addcontentsline{toc}{chapter}{Bibliografía}
\end{document}

```
